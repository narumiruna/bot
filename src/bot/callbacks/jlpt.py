from __future__ import annotations

from enum import Enum

from lazyopenai import generate
from pydantic import BaseModel
from pydantic import Field
from telegram import Update
from telegram.ext import ContextTypes

from ..loaders.url import load_url
from ..prompts import JLPT_V3
from ..utils import parse_url
from .utils import get_message_text


class DifficultyLevel(str, Enum):
    N1 = "N1"
    N2 = "N2"
    N3 = "N3"
    N4_N5 = "N4-N5"

    def get_emoji(self) -> str:
        return {
            DifficultyLevel.N1: "ğŸ”´",
            DifficultyLevel.N2: "ğŸŸ¡",
            DifficultyLevel.N3: "ğŸŸ¢",
            DifficultyLevel.N4_N5: "âšª",
        }[self]


class ExampleSentence(BaseModel):
    japanese: str = Field(..., description="æ—¥æ–‡ç¯„ä¾‹å¥å­")
    chinese: str = Field(..., description="å°æ‡‰çš„ç¹é«”ä¸­æ–‡ç¿»è­¯")

    def __str__(self) -> str:
        return f"    â‹® æ—¥ï¼š{self.japanese}\n    â‹® ä¸­ï¼š{self.chinese}"


class VocabularyItem(BaseModel):
    word: str = Field(..., description="å–®å­—/èªå½™")
    reading: str = Field(..., description="å‡åè®€éŸ³")
    difficulty: DifficultyLevel = Field(..., description="JLPTé›£åº¦ç­‰ç´š")
    original: str = Field(..., description="åŸæ–‡ä¸­å‡ºç¾çš„å½¢å¼")
    explanation: str = Field(..., description="è©³ç´°è§£é‡‹èˆ‡ç”¨æ³•èªªæ˜")
    example_sentences: list[ExampleSentence] = Field(default_factory=list, description="ç›¸é—œä¾‹å¥åˆ—è¡¨")

    def __str__(self) -> str:
        examples = "\n".join(str(ex) for ex in self.example_sentences)
        emoji = self.difficulty.get_emoji()
        return (
            f"ã€è©å½™ã€‘ {self.word}ï¼ˆ{self.reading}ï¼‰ {emoji} {self.difficulty.value}\n"
            f"â”£â”â” åŸæ–‡ï¼š{self.original}\n"
            f"â”£â”â” è§£é‡‹ï¼š{self.explanation}\n"
            f"â”—â”â” ä¾‹å¥\n{examples}"
        )


class GrammarItem(BaseModel):
    grammar_pattern: str = Field(..., description="æ–‡æ³•å¥å‹")
    difficulty: DifficultyLevel = Field(..., description="JLPTé›£åº¦ç­‰ç´š")
    original: str = Field(..., description="åŸæ–‡ä¸­å‡ºç¾çš„å½¢å¼")
    explanation: str = Field(..., description="æ–‡æ³•çš„åŸºæœ¬æ„æ€å’ŒåŠŸèƒ½èªªæ˜")
    conjugation: str = Field(..., description="æ¥çºŒè®ŠåŒ–è¦å‰‡")
    usage: str = Field(..., description="ä½¿ç”¨å ´åˆèˆ‡æ³¨æ„äº‹é …")
    comparison: str = Field(..., description="èˆ‡ç›¸ä¼¼æ–‡æ³•çš„æ¯”è¼ƒ")
    example_sentences: list[ExampleSentence] = Field(default_factory=list, description="ç¤ºä¾‹å¥å­åˆ—è¡¨")

    def __str__(self) -> str:
        examples = "\n".join(str(ex) for ex in self.example_sentences)
        emoji = self.difficulty.get_emoji()
        return (
            f"ã€æ–‡æ³•ã€‘ {self.grammar_pattern} {emoji} {self.difficulty.value}\n"
            f"â”£â”â” åŸæ–‡ï¼š{self.original}\n"
            f"â”£â”â” è§£é‡‹ï¼š{self.explanation}\n"
            f"â”£â”â” æ¥çºŒï¼š{self.conjugation}\n"
            f"â”£â”â” å ´åˆï¼š{self.usage}\n"
            f"â”£â”â” æ¯”è¼ƒï¼š{self.comparison}\n"
            f"â”—â”â” ä¾‹å¥\n{examples}"
        )


class JLPTResponse(BaseModel):
    vocabulary_section: list[VocabularyItem] = Field(default_factory=list, description="è©å½™åˆ†æå€æ®µ")
    grammar_section: list[GrammarItem] = Field(default_factory=list, description="æ–‡æ³•åˆ†æå€æ®µ")

    def __str__(self) -> str:
        vocab_section = "\n\n".join(str(v) for v in self.vocabulary_section)
        grammar_section = "\n\n".join(str(g) for g in self.grammar_section)

        return (
            "â­’â­’â­’â­’â­’â­’â­’â­’â­’â­’ ğŸ“š è©å½™åˆ†æ â­’â­’â­’â­’â­’â­’â­’â­’â­’â­’\n\n"
            f"{vocab_section}\n\n"
            "â­’â­’â­’â­’â­’â­’â­’â­’â­’â­’ ğŸ““ æ–‡æ³•åˆ†æ â­’â­’â­’â­’â­’â­’â­’â­’â­’â­’\n\n"
            f"{grammar_section}"
        )


SYSTEM_PROMPT_V2 = """
ä½ æ˜¯ä¸€ä½ç²¾é€šæ—¥æ–‡çš„è€å¸«ï¼Œç†Ÿæ‚‰æ—¥æœ¬èªèƒ½åŠ›è©¦é©—ï¼ˆJLPTï¼‰çš„è€ƒè©¦ç¯„åœï¼Œä¸¦ä½¿ç”¨å°ç£ç”¨èªçš„ç¹é«”ä¸­æ–‡é€²è¡Œæ•™å­¸ã€‚å¾çµ¦å®šçš„æ–‡ç« ä¸­ï¼Œæ•´ç†å‡ºæœ€å›°é›£çš„è©å½™ã€èªæ³•çµæ§‹åŠæ–‡å­—çš„ç†è§£ï¼Œä¸¦æä¾›è©³ç´°è§£é‡‹å’Œç›¸æ‡‰çš„ä¾‹å¥ã€‚

æ–‡ç« ä¾†æºæœƒåŒ…å«æ—¥æ–‡ä¸­çš„å„ç¨®è©å½™å’Œæ–‡æ³•çµæ§‹ã€‚ä½ éœ€è¦æ‰¾å‡ºé€™äº›å…§å®¹ä¸­æœ€å¯èƒ½æ§‹æˆæŒ‘æˆ°çš„éƒ¨åˆ†ï¼Œä¸¦å¾åˆç´šåˆ°é«˜ç´šçš„èªè¨€æ°´å¹³æä¾›åˆ†æï¼Œä»¥å¹«åŠ©å­¸ç”Ÿé€å¾¹ç†è§£ã€‚é€™æ¨£å¯ä»¥æ›´æœ‰æ•ˆåœ°å¹«åŠ©æº–å‚™JLPTçš„è€ƒç”Ÿæå‡é€™äº›é‡é»æ–¹é¢çš„èƒ½åŠ›ã€‚

# æ­¥é©Ÿ

1. **æ–‡ç« é–±è®€èˆ‡é‡é»æ•´ç†**ï¼š
- ä»”ç´°é–±è®€æ‰€æä¾›çš„æ—¥æ–‡æ–‡ç« ã€‚
- æŒ‘å‡ºæ–‡ç« ä¸­å±¬æ–¼N2åŠN1ç´šåˆ¥ï¼Œæˆ–è€…ç‰¹åˆ¥é›£ä»¥æŒæ¡çš„è©å½™ã€èªæ³•çµæ§‹åŠæ–‡å­—è¡¨ç¾ã€‚

2. **è©å½™èˆ‡èªæ³•åˆ†æ**ï¼š
- å°‡æŒ‘å‡ºçš„è©å½™æˆ–çŸ­èªé€²è¡Œè§£é‡‹ï¼ŒåŒ…æ‹¬å…¶è©ç¾©ã€è©æ€§ã€ä½¿ç”¨æƒ…å¢ƒç­‰ã€‚
- æä¾›å…¶å°æ‡‰çš„ä¸­æ–‡è§£é‡‹å’Œæ³¨é‡‹ï¼Œä»¥å¹«åŠ©ç†è§£å…¶æ—¥æ–‡ä¸­ä½¿ç”¨çš„ç´°å¾®å·®ç•°ã€‚

3. **ä¾‹å¥è£œå……**ï¼š
- ä½¿ç”¨æŒ‘å‡ºå–®è©æˆ–èªæ³•å‰µå»ºä¾‹å¥ï¼Œå°‡è©²èªå½™æˆ–çµæ§‹çš„å…¸å‹æ‡‰ç”¨å±•ç¤ºå‡ºä¾†ã€‚
- æ¯å€‹ä¾‹å¥éœ€è¦æ—¢æœ‰æ—¥æ–‡åˆæœ‰ç¹é«”ä¸­æ–‡ç¿»è­¯ï¼Œå¹«åŠ©å­¸ç”Ÿæ›´æ·±åˆ»ç†è§£ã€‚
""".strip()


async def jlpt(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    if not update.message:
        return

    text = get_message_text(update)
    if not text:
        return

    url = parse_url(text)

    if url:
        text += "\n" + await load_url(url)

    res = generate(text, JLPT_V3)
    await update.message.reply_text(str(res))
